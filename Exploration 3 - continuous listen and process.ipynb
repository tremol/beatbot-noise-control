{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we begin learning and exploring how to continuously listen for signals, that we might then pass through our trained neural net in order to recognize specific noises and act on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to use sounddevice: minimal working example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple instructive examples that use [pyaudio](https://people.csail.mit.edu/hubert/pyaudio/) can be found here:\n",
    "* https://github.com/swharden/Python-GUI-examples/blob/master/2016-07-37_qt_audio_monitor/SWHear.py\n",
    "* https://github.com/chaosparrot/parrot.py/blob/master/lib/listen.py\n",
    "\n",
    "The latter is a more complex example, but is part of a project similar to this one, and so may be particularly insightful. We will postpone trying to parse it until we need to, however. In particular, first let's understand the basics, and construct a minimal working example.\n",
    "\n",
    "An alternative library, which seems as powerful but much better documented, is [sounddevice](https://python-sounddevice.readthedocs.io/en/0.3.14/). This also has the benefit of outputting numpy arrays by default, which will save us some processing. We'll try to use sounddevice, but remember pyaudio as a fallback option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's construct a minimal working example. Here we are adapting and stripping down code from https://python-sounddevice.readthedocs.io/en/0.3.14/examples.html, a command-line script which shows a text-mode spectrogram using live microphone data. This listens for five seconds, printing out the maximum amplitude of each 50ms interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010467529\n",
      "-0.009124756\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "class args:\n",
    "    block_duration = 50 # ms\n",
    "    device = 2 # select the microphone. Use sd.query_devices() to see options\n",
    "\n",
    "samplerate = sd.query_devices(args.device, 'input')['default_samplerate']\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print('STATUS: ', str(status))\n",
    "    if any(indata):\n",
    "        # dynamically print the max and min values\n",
    "        clear_output(wait=True) # this sometimes takes too long, causing input overflows\n",
    "        print(indata.max())\n",
    "        print(indata.min())\n",
    "    else:\n",
    "        print('no input')\n",
    "\n",
    "start = time.time()\n",
    "with sd.InputStream(device=args.device, channels=1, callback=callback,\n",
    "                    blocksize=int(samplerate * args.block_duration / 1000),\n",
    "                    samplerate=samplerate):\n",
    "    while True:\n",
    "        # listen for five seconds\n",
    "        if time.time() - start > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another one to measure, how fast can I make sounds? Each sound is going to take at least 0.06s to listen and process, so let's hope I can't go faster than that. ... Well it looks like I easily can, at least with some sounds. We'll just hope the users go slow enough for now, and figure out how to handle over-rapid noisemaking later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11463308334350586\n",
      "0.09275507926940918\n",
      "0.005228996276855469\n",
      "0.0876607894897461\n",
      "0.005400180816650391\n",
      "0.08743906021118164\n",
      "0.015585660934448242\n",
      "0.07728719711303711\n",
      "0.01627182960510254\n",
      "0.07467126846313477\n",
      "0.04583382606506348\n",
      "0.05915117263793945\n",
      "0.035568952560424805\n",
      "0.04714703559875488\n",
      "0.09284710884094238\n",
      "0.02297663688659668\n",
      "0.07494711875915527\n",
      "0.040576934814453125\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD_ABSOLUTE = 0.005 # ignore any spikes that don't rise above this\n",
    "\n",
    "last_sound = time.time()\n",
    "\n",
    "def callback(indata, frames, time_pa, status):\n",
    "    global last_sound\n",
    "    if status:\n",
    "        print('STATUS: ', str(status))\n",
    "    if any(indata):\n",
    "        if indata.max() > THRESHOLD_ABSOLUTE:\n",
    "            new_sound = time.time()\n",
    "            print(new_sound - last_sound)\n",
    "            last_sound = new_sound\n",
    "    else:\n",
    "        print('no input')\n",
    "\n",
    "start = time.time()\n",
    "with sd.InputStream(device=args.device, channels=1, callback=callback,\n",
    "                    blocksize=int(samplerate * args.block_duration / 1000),\n",
    "                    samplerate=samplerate):\n",
    "    while True:\n",
    "        # listen for five seconds\n",
    "        if time.time() - start > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to real-time processing with noise recognition, let's see that we can load the model we trained in the Exploration 2 notebook, and successfully apply it to a 28x14 tensor.\n",
    "\n",
    "We need to copy paste the class here, and then load the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, image_size, N_noises):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # image_size is a 2-tuple, the expected dimensions of each spectrogram\n",
    "        channels, h, w = image_size\n",
    "        \n",
    "        # number of output nodes, (square) kernel size, and pool size per convolution layer,\n",
    "        # assuming the stride for pooling is the same as the pool size\n",
    "        kernels = [3, 3]\n",
    "        pool = 2\n",
    "        \n",
    "        # compute the number of input nodes for the first dense layer\n",
    "        h_out, w_out = h, w\n",
    "        for k in kernels:\n",
    "            # the convolution.\n",
    "            h_out += -k + 1\n",
    "            w_out += -k + 1\n",
    "            \n",
    "            # the pool. (from help(torch.nn.MaxPool2d))\n",
    "            h_out = int( (h_out - pool) / pool + 1 )\n",
    "            w_out = int( (w_out - pool) / pool + 1 )\n",
    "            \n",
    "        self.image_out = h_out * w_out\n",
    "        \n",
    "        # define the layers. The numbers of nodes chosen do not have deep thought behind them.\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernels[0])\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv1 = nn.Conv2d(32, 10, kernels[1])\n",
    "        self.fc0 = nn.Linear(10 * self.image_out, 50)\n",
    "        self.fc1 = nn.Linear(50, 10)\n",
    "        # number of output nodes for final dense layer: the number of noise types        \n",
    "        self.fc2 = nn.Linear(10, N_noises)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv0(x)))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 10 * self.image_out)\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Net(torch.Size([1, 28, 14]), 15)\n",
    "\n",
    "# Now load the parameters\n",
    "PATH2 = './trained_models/14_noises_60ms_model_params.pth'\n",
    "model.load_state_dict(torch.load(PATH2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And get the dictionary of noise labels\n",
    "noise_int_to_str = {\n",
    "    0: 't',\n",
    "    1: 'p',\n",
    "    2: 'k',\n",
    "    3: 'ch',\n",
    "    4: 'ts',\n",
    "    5: 'ps',\n",
    "    6: 'ks',\n",
    "    7: 'chsh',\n",
    "    8: 'tf',\n",
    "    9: 'pf',\n",
    "    10: 'kf',\n",
    "    11: 'chf',\n",
    "    12: 'forward-tsk',\n",
    "    13: 'side-cluck',\n",
    "    14: 'lip-open-pop'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took time: 0.0010840892791748047\n",
      "1.2322986125946045\n",
      "13\n",
      "side-cluck\n"
     ]
    }
   ],
   "source": [
    "# Test model on dummy data.\n",
    "# Remember the models except many batches, so the first \n",
    "# dimension is the batch size (the second is the number of channels)\n",
    "\n",
    "foo = torch.rand(1,1,28,14) \n",
    "start = time.time()\n",
    "output = model(foo)\n",
    "print('This took time:', time.time() - start)\n",
    "energy, label = [ x.item() for x in torch.max(output.data, 1) ]\n",
    "print(energy)\n",
    "print(label)\n",
    "print(noise_int_to_str[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an interesting observation here, it seems that random spectrograms pretty much always produce 'side-cluck'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the neural net for real-time recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's adopt this basic template to listen for spikes in volume, listen for a sufficient period, generate a spectrogram, classify the sound with the neural net we trained, and print the result.\n",
    "\n",
    "At the moment, this is fairly fragile, in the sense that we have to carefully process this audio in the same way that we processed audio for the neural network, in \"Exploration 2 - many noises - cleaning and training.ipynb\" (we saved the resulting network as ./trained_models/14_noises_net1.pth). To this end, we have carefully copied over here some key parameters. This is sufficient for now, but ultimately we will want to make this more robust, especially to allow for varying the neural network approach without having to manually synchronize this code to match.\n",
    "\n",
    "There are three key features that we should preserve, since these were used in training the net:\n",
    "* Each audio sample should produce a Mel Spectrogram that is 28x14 in resolution. The 28 is the number of mel filterbanks, which is easily specified. The 14 is set by the duration of the sample analyzed.\n",
    "* The samples should be identified with the percussive sound starting near the beginning of the sample.\n",
    "* The samples should be normalized by dividing out the mean amplitude.\n",
    "\n",
    "The first feature is the most important, because otherwise the net will throw an error. We think the '14' dimension comes about as follows: There were AFTER * frame_rate = 3 * 0.02 * 44100 = 2646 frames, grouped into windows of width 400, with each window a hop of 200 frames over from the one before, hence ceil(2646 / 200) = 14. These are the default values for the window width and hop, and can be found here: https://pytorch.org/audio/transforms.html#melspectrogram. We will just try to gather the same number of frames per sample to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "MelSpectrogram 0.13785767555236816\n",
      "side-cluck\n",
      "Processing took 0.41373276710510254 sec\n",
      "\n",
      "STATUS:  input overflow\n",
      "STATUS:  input overflow\n",
      "STATUS:  input overflow\n",
      "STATUS:  input overflow\n",
      "STATUS:  input overflow\n",
      "MelSpectrogram 0.15822410583496094\n",
      "k\n",
      "Processing took 0.4182741641998291 sec\n",
      "\n",
      "STATUS:  input overflow\n",
      "STATUS:  input overflow\n",
      "STATUS:  input overflow\n",
      "STATUS:  input overflow\n",
      "MelSpectrogram 0.14787721633911133\n",
      "ts\n",
      "Processing took 0.4228239059448242 sec\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "########### parameters ###########\n",
    "\n",
    "device = 2 # select the microphone. Use sd.query_devices() to see options\n",
    "\n",
    "# These are key variables and quantities we used in training the network.\n",
    "BATCH_DURATION = 0.02 # look at BATCH_DURATION (seconds) at a time\n",
    "THRESHOLD_MULTIPLIER = 5 # detect a spike when the next batch is at least THRESHOLD_MULTIPLIER times bigger\n",
    "# AFTER  = 3 * BATCH_DURATION # the time (sec) to look after the spike location\n",
    "n_mels = 28 # the number of mel filterbanks in the spectrogram\n",
    "# --------------------\n",
    "\n",
    "THRESHOLD_ABSOLUTE = 0.005 # ignore any spikes that don't rise above this\n",
    "\n",
    "samplerate = sd.query_devices(args.device, 'input')['default_samplerate']\n",
    "\n",
    "block_duration = BATCH_DURATION # sec\n",
    "blocksize = int(samplerate * block_duration) # convert to frames\n",
    "\n",
    "#################################\n",
    "\n",
    "# Lots of debugging time checks in here right now\n",
    "def get_prediction(recording):\n",
    "    \"\"\" Build the spectrogram and use our model to recognize the noise \"\"\"\n",
    "    \n",
    "#     now = time.time()\n",
    "    obs_data = torch.from_numpy(recording) / recording.mean()\n",
    "#     print('obs_data = torch.from_numpy(recording)', time.time() - now)\n",
    "\n",
    "    now = time.time()\n",
    "    mel = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=samplerate, n_mels=n_mels)(obs_data).log2()\n",
    "    print('MelSpectrogram', time.time() - now)\n",
    "    \n",
    "#     now = time.time()\n",
    "    # change from torch.Size([28, 14]) to torch.Size([1, 1, 28, 14])\n",
    "    mel = mel[None, None, :, :]\n",
    "#     print('mel = mel[None, None, :, :]', time.time() - now)\n",
    "\n",
    "#     now = time.time()\n",
    "    output = model(mel)\n",
    "#     print('output = model(mel)', time.time() - now)\n",
    "\n",
    "#     now = time.time()\n",
    "    energy, label = torch.max(output.data, 1)\n",
    "#     print('energy, label = torch.max(output.data, 1)', time.time() - now)\n",
    "    \n",
    "    return noise_int_to_str[label.item()]\n",
    "\n",
    "def act_on_noise(noise_heard):\n",
    "    print(noise_heard)\n",
    "\n",
    "# bundling these is easier than declaring them 'global' in callback\n",
    "class listen:\n",
    "    prev_max = 1\n",
    "    sound_detected = False\n",
    "    batches_to_collect = 0\n",
    "    recording = None\n",
    "    start = 0\n",
    "    end = 0\n",
    "    \n",
    "def callback(indata, frames, time_pa, status):\n",
    "    if status:\n",
    "        print('STATUS: ', str(status))\n",
    "    if any(indata):\n",
    "        new_max = np.absolute(indata).max()\n",
    "                \n",
    "        if listen.sound_detected:\n",
    "            if listen.batches_to_collect > 0:\n",
    "                listen.recording = np.append( listen.recording, indata )\n",
    "                listen.batches_to_collect -= 1\n",
    "            else:\n",
    "                noise_heard = get_prediction(listen.recording)\n",
    "                act_on_noise(noise_heard)\n",
    "                listen.sound_detected = False\n",
    "                listen.end = time.time()\n",
    "                print('Processing took', listen.end - listen.start, 'sec\\n')\n",
    "                \n",
    "        elif ( new_max > THRESHOLD_ABSOLUTE and\n",
    "               new_max > THRESHOLD_MULTIPLIER * listen.prev_max ):\n",
    "#             print('noise', new_max, '<', listen.prev_max)\n",
    "            listen.start = time.time()\n",
    "            listen.sound_detected = True\n",
    "            listen.batches_to_collect = 2 # get two more batches, because AFTER = 3 batches\n",
    "            listen.recording = indata[:]\n",
    "            \n",
    "        listen.prev_max = new_max\n",
    "        \n",
    "    else:\n",
    "        print('no input')\n",
    "\n",
    "start = time.time()\n",
    "with sd.InputStream(device=device, channels=1, callback=callback,\n",
    "                    blocksize=int(samplerate * block_duration),\n",
    "                    samplerate=samplerate):\n",
    "    print('Listening...')\n",
    "    while True:\n",
    "        # listen for a few seconds\n",
    "        if time.time() - start > 3:\n",
    "            break\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, this doesn't perform very well. The processing is quite slow, and the recognition doesn't seem very good, despite the excellent (95%) testing performance the model displayed when we first built it. The spectrogram calculation seems to be the biggest single time sink (at least in the get_prediction function), but only one third of the total time. We end up with a lot of input overflow errors also, because the callback function takes so long that it can't keep up with the rate of audio input.\n",
    "\n",
    "However, at least a big part of the problem seems to be something going on with the performance of the audio stream and callback mechanism, not necessarily the content of the callback function. For instance, reprocessing the audio from the last recognized sound, we see things go orders of magnitude faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MelSpectrogram 0.0020189285278320312\n",
      "ts\n",
      "Total time elapsed: 0.00464177131652832\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "foo = torch.from_numpy(listen.recording) / listen.recording.mean()\n",
    "\n",
    "now = time.time()\n",
    "mel = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=samplerate, n_mels=n_mels)(foo).log2()\n",
    "print('MelSpectrogram', time.time() - now)\n",
    "\n",
    "mel = mel[None, None, :, :]\n",
    "output = model(mel)\n",
    "energy, label = torch.max(output.data, 1)\n",
    "print(noise_int_to_str[label.item()])\n",
    "\n",
    "print('Total time elapsed:', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, now that we have the essential features in place, we can work on tuning up performance. We'll focus on making it go fast first, and then worry about accuracy. If the recordings are somehow getting distorted by a callback function that can't keep up, it won't matter how well-trained our model is.\n",
    "\n",
    "Studying some more of the real-time examples in the sounddevice documentation is likely a good place to start: https://python-sounddevice.readthedocs.io/en/0.3.14/examples.html#plot-microphone-signal-s-in-real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
