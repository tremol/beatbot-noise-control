{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In this notebook we begin learning and exploring how to continuously listen for signals, that we might then pass through our trained neural net in order to recognize specific noises and act on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to use sounddevice: minimal working example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple instructive examples that use [pyaudio](https://people.csail.mit.edu/hubert/pyaudio/) can be found here:\n",
    "* https://github.com/swharden/Python-GUI-examples/blob/master/2016-07-37_qt_audio_monitor/SWHear.py\n",
    "* https://github.com/chaosparrot/parrot.py/blob/master/lib/listen.py\n",
    "\n",
    "The latter is a more complex example, but is part of a project similar to this one, and so may be particularly insightful. We will postpone trying to parse it until we need to, however. In particular, first let's understand the basics, and construct a minimal working example.\n",
    "\n",
    "An alternative library, which seems as powerful but much better documented, is [sounddevice](https://python-sounddevice.readthedocs.io/en/0.3.14/). This also has the benefit of outputting numpy arrays by default, which will save us some processing. We'll try to use sounddevice, but remember pyaudio as a fallback option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's construct a minimal working example. Here we are adapting and stripping down code from https://python-sounddevice.readthedocs.io/en/0.3.14/examples.html, a command-line script which shows a text-mode spectrogram using live microphone data. This listens for five seconds, printing out the maximum amplitude of each 50ms interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010467529\n",
      "-0.009124756\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "class args:\n",
    "    block_duration = 50 # ms\n",
    "    device = 2 # select the microphone. Use sd.query_devices() to see options\n",
    "\n",
    "samplerate = sd.query_devices(args.device, 'input')['default_samplerate']\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print('STATUS: ', str(status))\n",
    "    if any(indata):\n",
    "        # dynamically print the max and min values\n",
    "        clear_output(wait=True) # this sometimes takes too long, causing input overflows\n",
    "        print(indata.max())\n",
    "        print(indata.min())\n",
    "    else:\n",
    "        print('no input')\n",
    "\n",
    "start = time.time()\n",
    "with sd.InputStream(device=args.device, channels=1, callback=callback,\n",
    "                    blocksize=int(samplerate * args.block_duration / 1000),\n",
    "                    samplerate=samplerate):\n",
    "    while True:\n",
    "        # listen for five seconds\n",
    "        if time.time() - start > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another one to measure, how fast can I make sounds? Each sound is going to take at least 0.06s to listen and process, so let's hope I can't go faster than that. ... Well it looks like I easily can, at least with some sounds. We'll just hope the users go slow enough for now, and figure out how to handle over-rapid noisemaking later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11463308334350586\n",
      "0.09275507926940918\n",
      "0.005228996276855469\n",
      "0.0876607894897461\n",
      "0.005400180816650391\n",
      "0.08743906021118164\n",
      "0.015585660934448242\n",
      "0.07728719711303711\n",
      "0.01627182960510254\n",
      "0.07467126846313477\n",
      "0.04583382606506348\n",
      "0.05915117263793945\n",
      "0.035568952560424805\n",
      "0.04714703559875488\n",
      "0.09284710884094238\n",
      "0.02297663688659668\n",
      "0.07494711875915527\n",
      "0.040576934814453125\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD_ABSOLUTE = 0.005 # ignore any spikes that don't rise above this\n",
    "\n",
    "last_sound = time.time()\n",
    "\n",
    "def callback(indata, frames, time_pa, status):\n",
    "    global last_sound\n",
    "    if status:\n",
    "        print('STATUS: ', str(status))\n",
    "    if any(indata):\n",
    "        if indata.max() > THRESHOLD_ABSOLUTE:\n",
    "            new_sound = time.time()\n",
    "            print(new_sound - last_sound)\n",
    "            last_sound = new_sound\n",
    "    else:\n",
    "        print('no input')\n",
    "\n",
    "start = time.time()\n",
    "with sd.InputStream(device=args.device, channels=1, callback=callback,\n",
    "                    blocksize=int(samplerate * args.block_duration / 1000),\n",
    "                    samplerate=samplerate):\n",
    "    while True:\n",
    "        # listen for five seconds\n",
    "        if time.time() - start > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to real-time processing with noise recognition, let's see that we can load the model we trained in the Exploration 2 notebook, and successfully apply it to a 28x14 tensor.\n",
    "\n",
    "We need to copy paste the class here, and then load the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, image_size, N_noises):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # image_size is a 2-tuple, the expected dimensions of each spectrogram\n",
    "        channels, h, w = image_size\n",
    "        \n",
    "        # number of output nodes, (square) kernel size, and pool size per convolution layer,\n",
    "        # assuming the stride for pooling is the same as the pool size\n",
    "        kernels = [3, 3]\n",
    "        pool = 2\n",
    "        \n",
    "        # compute the number of input nodes for the first dense layer\n",
    "        h_out, w_out = h, w\n",
    "        for k in kernels:\n",
    "            # the convolution.\n",
    "            h_out += -k + 1\n",
    "            w_out += -k + 1\n",
    "            \n",
    "            # the pool. (from help(torch.nn.MaxPool2d))\n",
    "            h_out = int( (h_out - pool) / pool + 1 )\n",
    "            w_out = int( (w_out - pool) / pool + 1 )\n",
    "            \n",
    "        self.image_out = h_out * w_out\n",
    "        \n",
    "        # define the layers. The numbers of nodes chosen do not have deep thought behind them.\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernels[0])\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv1 = nn.Conv2d(32, 10, kernels[1])\n",
    "        self.fc0 = nn.Linear(10 * self.image_out, 50)\n",
    "        self.fc1 = nn.Linear(50, 10)\n",
    "        # number of output nodes for final dense layer: the number of noise types        \n",
    "        self.fc2 = nn.Linear(10, N_noises)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv0(x)))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 10 * self.image_out)\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Net(torch.Size([1, 28, 14]), 15)\n",
    "\n",
    "# Now load the parameters\n",
    "PATH2 = './trained_models/14_noises_60ms_model_params.pth'\n",
    "model.load_state_dict(torch.load(PATH2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And get the dictionary of noise labels\n",
    "noise_int_to_str = {\n",
    "    0: 't',\n",
    "    1: 'p',\n",
    "    2: 'k',\n",
    "    3: 'ch',\n",
    "    4: 'ts',\n",
    "    5: 'ps',\n",
    "    6: 'ks',\n",
    "    7: 'chsh',\n",
    "    8: 'tf',\n",
    "    9: 'pf',\n",
    "    10: 'kf',\n",
    "    11: 'chf',\n",
    "    12: 'forward-tsk',\n",
    "    13: 'side-cluck',\n",
    "    14: 'lip-open-pop'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2102808952331543\n",
      "13\n",
      "side-cluck\n",
      "CPU times: user 2.61 ms, sys: 1.26 ms, total: 3.87 ms\n",
      "Wall time: 2.72 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test model on dummy data.\n",
    "# Remember the models except many batches, so the first \n",
    "# dimension is the batch size (the second is the number of channels)\n",
    "\n",
    "foo = torch.rand(1,1,28,14) \n",
    "output = model(foo)\n",
    "energy, label = [ x.item() for x in torch.max(output.data, 1) ]\n",
    "print(energy)\n",
    "print(label)\n",
    "print(noise_int_to_str[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an interesting observation here, it seems that random spectrograms pretty much always produce 'side-cluck'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the neural net for real-time recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's adopt this basic template to listen for spikes in volume, listen for a sufficient period, generate a spectrogram, classify the sound with the neural net we trained, and print the result.\n",
    "\n",
    "At the moment, this is fairly fragile, in the sense that we have to carefully process this audio in the same way that we processed audio for the neural network, in \"Exploration 2 - many noises - cleaning and training.ipynb\" (we saved the resulting network parameters as ./trained_models/14_noises_60ms_model_params.pth). To this end, we have carefully copied over here some key parameters. This is sufficient for now, but ultimately we like to make this more robust, especially to allow for varying the neural network approach without having to manually synchronize this code to match.\n",
    "\n",
    "There are three key features that we should preserve, since these were used in training the net:\n",
    "* Each audio sample should produce a Mel Spectrogram that is 28x14 in resolution. The 28 is the number of mel filterbanks, which is easily specified. The 14 is set by the duration of the sample analyzed.\n",
    "* The samples should be identified with the percussive sound starting near the beginning of the sample, for instance using the same spike identification tactic we used for the net.\n",
    "* The samples should be normalized by dividing out the mean amplitude, before taking the spectrogram.\n",
    "\n",
    "The first feature is the most important, because otherwise the net will throw an error, but the predictions will only be sensible of the other two are met also. We think the '14' dimension comes about as follows: There were AFTER * frame_rate = 3 * 0.02 * 44100 = 2646 frames, grouped into windows of width 400, with each window a hop of 200 frames over from the one before, hence ceil(2646 / 200) = 14. These are the default values for the window width and hop, and can be found here: https://pytorch.org/audio/transforms.html#melspectrogram. We will just try to gather the same number of frames per sample to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with setting these parameters, and defining the callback function to listen for volume spikes and add the audio to a queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import numpy as np\n",
    "import queue\n",
    "\n",
    "########### parameters ###########\n",
    "\n",
    "device = 2 # select the microphone. Use sd.query_devices() to see options\n",
    "\n",
    "# These are key variables and quantities we used in training the network.\n",
    "BATCH_DURATION = 0.02 # look at BATCH_DURATION (seconds) at a time\n",
    "THRESHOLD_MULTIPLIER = 5 # detect a spike when the next batch is at least THRESHOLD_MULTIPLIER times bigger\n",
    "# AFTER  = 3 * BATCH_DURATION # the time (sec) to look after the spike location\n",
    "BATCHES_PER_RECORDING = 3 # This isn't copied from the model, but will play the role that AFTER played in the model\n",
    "n_mels = 28 # the number of mel filterbanks in the spectrogram\n",
    "# --------------------\n",
    "\n",
    "THRESHOLD_ABSOLUTE = 0.005 # ignore any spikes that don't rise above this. Too many false positives without this\n",
    "\n",
    "samplerate = sd.query_devices(device, 'input')['default_samplerate']\n",
    "assert int(samplerate) == 44100, 'The sample rate should be 44100 for now.'\n",
    "# We used a sample rate of 44100 in the audio for training and testing the model.\n",
    "# This matters for the window size and frequency sensitivity of the FFT used in spectrograms.\n",
    "\n",
    "blocksize = int(samplerate * BATCH_DURATION) # get the block (batch) size in frames\n",
    "\n",
    "#################################\n",
    "\n",
    "# bundling these is easier than declaring them 'global' in callback\n",
    "class listen:\n",
    "    prev_max = 1\n",
    "    batches_to_collect = 0\n",
    "    batches_collected = 0\n",
    "    recording = None\n",
    "    \n",
    "    start = 0 # for timing the total processing time\n",
    "    end = 0\n",
    "    \n",
    "q_recordings = queue.Queue()\n",
    "    \n",
    "def callback(indata, frames, time_pa, status):\n",
    "    \"\"\" Detect if a noise has been made, and add audio to the queue. \"\"\"\n",
    "    if status:\n",
    "        print('STATUS: ', str(status))\n",
    "    if any(indata):\n",
    "        new_max = np.absolute(indata).max()\n",
    "        \n",
    "        # Gather audio data if more is required\n",
    "        if listen.batches_to_collect > 0:\n",
    "            q_recordings.put_nowait(indata[:])\n",
    "            listen.batches_collected  += 1\n",
    "            listen.batches_to_collect -= 1\n",
    "            return\n",
    "                \n",
    "        # See if a new noise has been detected\n",
    "        elif ( new_max > THRESHOLD_ABSOLUTE and\n",
    "             new_max > THRESHOLD_MULTIPLIER * listen.prev_max ):\n",
    "            \n",
    "            listen.start = time.time()\n",
    "            \n",
    "            q_recordings.put_nowait(indata[:])\n",
    "            listen.batches_collected += 1\n",
    "            listen.batches_to_collect = BATCHES_PER_RECORDING - 1 # get more batches\n",
    "               \n",
    "        listen.prev_max = new_max\n",
    "        \n",
    "    else:\n",
    "        print('no input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use this callback function in the sounddevice stream just to queue up audio, and then process it in the main loop in the stream 'with' block. We tried doing this processing in the callback function itself, but that proved to be far too slow---in fact it took a couple orders of magnitude longer, for some reason, taking up to half a second to process a 60ms noise.\n",
    "\n",
    "Now let's define the main function to run and process the audio stream. We leave the specific processing functions agnostic for now. With our current approach we wish to compute spectrograms and run the audio through our model to recognize noises, and perform actions according to which noise is heard. However, we might also wish to do other things, like just save the noises that are heard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listen_and_process(duration, processing_function, device=device):\n",
    "    \"\"\" Listen continuously for noises for duration (sec), then process them using processing_function \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    with sd.InputStream(device=device, channels=1, callback=callback,\n",
    "                        blocksize=blocksize,\n",
    "                        samplerate=samplerate):\n",
    "        print('Listening...')\n",
    "        while True:\n",
    "            \n",
    "            # data collects if it meets the threshold. Process if enough data is in queue\n",
    "            if listen.batches_collected >= BATCHES_PER_RECORDING:\n",
    "                data = []\n",
    "                for _ in range(BATCHES_PER_RECORDING):\n",
    "                    data.append( q_recordings.get_nowait() )\n",
    "                       \n",
    "                listen.batches_collected -= BATCHES_PER_RECORDING\n",
    "                listen.recording = np.concatenate( data, axis=None )\n",
    "                \n",
    "                processing_function( listen.recording )\n",
    "                \n",
    "                listen.end = time.time()\n",
    "                print('Processing took {:.4f} sec\\n'.format(listen.end - listen.start))\n",
    "            \n",
    "            # listen for a few seconds total\n",
    "            if time.time() - start > duration:\n",
    "                break\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recognize the noises with our model, and print the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "pf\n",
      "Processing took 0.0846 sec\n",
      "\n",
      "pf\n",
      "Processing took 0.0949 sec\n",
      "\n",
      "t\n",
      "Processing took 0.0021 sec\n",
      "\n",
      "pf\n",
      "Processing took 0.0020 sec\n",
      "\n",
      "pf\n",
      "Processing took 0.0021 sec\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "def get_prediction(recording):\n",
    "    \"\"\" Build the spectrogram and use our model to recognize the noise \"\"\"\n",
    "    \n",
    "    # normalize like we did in training the model, and compute the spectrogram\n",
    "    obs_data = torch.from_numpy(recording) / recording.mean()\n",
    "    mel = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=samplerate, n_mels=n_mels)(obs_data).log2()\n",
    "\n",
    "    # change from torch.Size([28, 14]) to torch.Size([1, 1, 28, 14])\n",
    "    mel = mel[None, None, :, :]\n",
    "    \n",
    "    # run through the model and get prediction\n",
    "    output = model(mel)\n",
    "    energy, label = torch.max(output.data, 1)\n",
    "    \n",
    "    # return the string label of the noise\n",
    "    return noise_int_to_str[label.item()]\n",
    "\n",
    "def print_noise(noise_heard):\n",
    "    \"\"\" Print the string label of the noise that has been heard. \"\"\"\n",
    "    print(noise_heard)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "listen_and_process(3, lambda rec: print_noise(get_prediction(rec)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing speed is excellent, but the accuracy seems terrible, contrary to the excellent testing results when we first built the model. Additionally, we sometimes observed that several noises are detected for each one that we make.\n",
    "\n",
    "Let's record some noises and play them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Processing took 0.0003 sec\n",
      "\n",
      "Processing took 0.0931 sec\n",
      "\n",
      "Processing took 0.0879 sec\n",
      "\n",
      "Processing took 0.0003 sec\n",
      "\n",
      "Processing took 0.0002 sec\n",
      "\n",
      "Processing took 0.0726 sec\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "recordings = []\n",
    "def gather_recordings(recording):\n",
    "    \"\"\" Just remember the recordings \"\"\"\n",
    "    recordings.append(recording)\n",
    "\n",
    "def playback_recordings(recordings):\n",
    "    for rec in recordings:\n",
    "        sd.play(rec / rec.max() / 10, samplerate)\n",
    "        sd.wait()\n",
    "\n",
    "listen_and_process(3, gather_recordings)\n",
    "playback_recordings(recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, excellent, the audio being gathered is total nonsense. That's why the recognition is terrible. Let's borrow some example code from the sounddevice documentation, to simultaneously record and playback audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0 Built-in Microphone, Core Audio (2 in, 0 out)\n",
       "< 1 Built-in Output, Core Audio (0 in, 2 out)\n",
       "> 2 SpeechMatic USB MultiAdapter, Core Audio (1 in, 2 out)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check our input and output devices\n",
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "press Return to quit\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Pass input directly to output.\n",
    "\n",
    "See https://www.assembla.com/spaces/portaudio/subversion/source/HEAD/portaudio/trunk/test/patest_wire.c\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cumulated_status = sd.CallbackFlags()\n",
    "\n",
    "input_device  = 2\n",
    "output_device = 1\n",
    "\n",
    "def callback2(indata, outdata, frames, time, status):\n",
    "    global cumulated_status\n",
    "    cumulated_status |= status\n",
    "    outdata[:] = indata\n",
    "\n",
    "with sd.Stream(device=(input_device, output_device),\n",
    "               samplerate=samplerate, blocksize=blocksize,\n",
    "               channels=(1,2), callback=callback2):\n",
    "    print(\"#\" * 80)\n",
    "    print(\"press Return to quit\")\n",
    "    print(\"#\" * 80)\n",
    "    input()\n",
    "\n",
    "if cumulated_status:\n",
    "    logging.warning(str(cumulated_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works excellently, and the audio is very sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
